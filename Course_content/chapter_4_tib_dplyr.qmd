---
title: "Chapter 4 - Tibbles and dplyr"
format:
  html:
    highlight: null
    theme: default
    toc: true
    toc-title: Contents
    toc-location: right
    toc-depth: 3
    number-sections: true 
    link-external-newwindow: true
    embed-resources: true
---

![](../Images/AF_DSC_banner.png){fig-alt="Data Science Campus and Analysis Function logos."}

# Learning Objectives

* Understand the importance of clean variable names.

* Be able to clean column names using the janitor package.

* Understand the use of the pipe operator.

* Be able to sort data with dplyr's **arrange** verb.

* Be able to select data with dplyr's **select** verb.

* Be able to filter data with dplyr's **filter** verb. 

* Be able to transform data with dplyr's **mutate** verb.

* Be able to join datasets together.


# Packages and Data

Remember, the first steps when starting a new script are:

* Load in the packages required for the work.
* Read in datasets required and assign them to a variable in memory.

## Exercise
::: {.panel-tabset}

### **Exercise**{-}

1. Load the following packages:

* Tidyverse
* janitor

2. Read in the **titanic.csv** file and assign it to the name "titanic_data". Remember to assign null values properly (as in Chapter 3) using the "na" parameter. 

* Remember that you are in your R project, which sets the working directory to be inside the **Course_content** folder. 

3. Have a glimpse of your dataset. 


### **Show Answer**{-}

```{r message = FALSE, warning = FALSE}

# Load packages

library(tidyverse)
library(janitor)


# Read in titanic.csv and set null values to be specific symbols

titanic_data <- read_csv("../Data/titanic.csv", 
                           na = c("*", ".", "", "NULL"))

# Have a peak

glimpse(titanic_data)

```

:::


As a reminder, in the titanic dataset our columns are:

* **Pclass**: Passengerâ€™s class, 1 = 1st (Upper), 2 = 2nd(Middle), 3 = 3rd(Lower)
* **Survived**: Survived (1) or died (0)
* **Name**: Passengerâ€™s name
* **Sex**: Passengerâ€™s sex
* **Age of Passenger**: Passengerâ€™s age
* **SibSp**: Number of siblings/spouses aboard (excluding the person)
* **Parch**: Number of parents/children aboard (excluding the person)
* **Ticket**: Ticket number
* **Fare**: Fare
* **Cabin**: Cabin number
* **Embarked**: Port of embarkation, C = Cherbourg, Q = Queenstown, S = Southampton


We can see more details on the [Data Dictionary](https://www.kaggle.com/c/titanic/data)


# Column Names

In the previous session we stated that every column in a tibble is a variable and it is good practice to not have spaces within variable names, as spaces makes it harder for us to call on the variables when we need to use them.

When you enter data in Excel, you most often donâ€™t think too much about what you call each column. After all, you just label them once and as long as they are documented, this isn't given too much thought.

When you are working with variables in R though, you need to type the name of each variable, every time you want to work with it. So, it makes sense to make your column names as simple, but meaningful as possible.

## Retuning columns by name

In base R, to call a column by name and return its contents as a single vector (remember, each column in a tibble is a vector) we use the dollar sign **$** operator.

You will notice the list of column names will pop up and you can move through them with arrow keys and select the one you want. 

### Example

Let's return the column "Pclass".

```{r}
# Return Pclass with base R

titanic_data$Pclass

```

This returns up to 1000 length vectors, so it would be useful to use **glimpse()** or other inspection functions for a sense check.

```{r}
# Return Pclass and glimpse

glimpse(titanic_data$Pclass)

```


This could already prove frustrating due to needing to remember the capital letters, particularly if the autocomplete is slowing down.

However, it can get worse if spaces are included in the column name.

### Example - Returning column with spaces

Let's take the "name of Passenger" column and try to return it without the auto-complete.

```{r, eval = FALSE}

titanic$name Of Passenger

```

This will throw an error as spaces in syntax are not allowed, R cannot process code in this way as a space should usually denote the end of a line of code. 

To get around this we enclose column names with spaces in **backticks ` `** and you will notice that the autocomplete does the same. This allows the entire column name to be read as one entity.

```{r}
# Selecting a column with spaces in the names

glimpse(titanic$`name Of Passenger`)

```

Whilst this works, it is bad practice to use capitalisation and spaces, as it complicates things for us as well as others we collaborate with.

### Accessing column names

We can see the column names by using the "names()" function to print a character vector of the column names.

```{r}
# Getting the column names using the names function

names(titanic)
```

We will need to do some work on these to remove the use of capitalisation and spaces. 


## Cleaning Column Names
::: {.panel-tabset}

### **janitor Package**{-}

The **janitor** package offers many functions used to manipulate data, such as finding duplicates. In this chapter we will use it to clean column names. 

The function to use is called "clean_names()" and automatically formats the column names as **snake_case**, but this can be altered with an additional parameter.


```{r}
# Clean the column names and overwrite the variable

titanic_data <- clean_names(titanic_data)

# Getting the column names of the dataset

names(titanic_data)
```
 
### **gsub**{-}

If you want to do this using base R, there is the function **gsub()** which is part of the pattern matching family of functions.

These are for use with character strings, and search for a specific pattern of characters and perform some task, be it:

* Replacement with another pattern
* Removal

and more. 

The function takes a few arguments:

* pattern - The sequence of characters to search the string for.
* replacement - What to replace the sequence of characters with.
* x - The string itself to search.

> For example, the output of gsub(" ", "_", "piece of text") would be "piece_of_text", as we are taking empty spaces, replacing them with underscores.

Let's perform the same steps as clean_names() using this function. Notice that we must overwrite the names() vector output, so it is less user friendly than janitor. 

```{r, eval = F}
# Using gsub to clean our column names

names(titanic_data) <- gsub(pattern = " ",  
                            replacement = "_", 
                            x = names(titanic_data)) 
```

That's not all though, we also need to lowercase them as well, using another base R function, the **tolower()** function.

```{r, eval = False}
# We can lower case the names of the data frame.

names(titanic_data) <- tolower(names(titanic_data)) 
```

Whichever you use depends on your setup and preferences. Janitor is the simplest and most popular, but provides less fine control than doing things step by step, so keep this in mind.

:::

### **dplyr**{-}

We have not formally introduced the data manipulation package dplyr, but it does have 

We can also rename column names. 

The first argument is the data frame to be changed, the second is the column and what it will be changed to.



```{r}
# Renaming a column using the package dplyr
# using the rename() function
# here we are renaming age_of_passenger with age

titanic_renamed <- dplyr::rename(titanic, age = age_of_passenger)

# To Display the Data

titanic_renamed

```



# The Data Manipulation Package - **dplyr**

This is one of the most powerful packages in the *tidyverse*, which makes data manipulation simple and code easy to read.

We will look at how to perform the following actions:

1. arrange/sort
2. select
3. filter
4. mutate
5. joining data

with the aim of the package to provide a function for each basic **verb** of data manipulation. This has led to them being referred to as such in the documentation. 

Each of the verbs have the same structure:

> verb(.data, info,....), note the full stop which is syntax to allow us to reference variables from the dataset (enables auto-completion too!)

and the [cheat sheet](https://nyu-cdsc.github.io/learningr/assets/data-transformation.pdf) is incredibly useful for a reference piece. 


## Arrange

Our data is displayed in the same order as the source data.

We may want to sort our data, based on specific columns. To do so we use the verb **arrange()**. 


### Example - Single column sort

For many of these examples we will create a new variable, as we may not want to overwrite our original data with cleaned names unless we are happy with a permanent change.

It makes it difficult to revert to the original data should we make a mistake if we do not create separate variables. 

However, we must also be careful about cluttering up our environment with variables we will not use again. 

As such, you can run the code in the console to check whether it gives the output you want, and then create a variable once you are sure. 

```{r}
# Sort titanic by fare

titanic_sorted <- arrange(.data = titanic_data,
                          fare)

glimpse(titanic_sorted)

```

Using glimpse() is highly recommended after each transformation to check that the output is what you expect.

Notice that by default, arrange() sorted the fare column in **ascending** order.


### Example - Desending Order Sort

To sort a column in descending order, we use the **desc()** function and the column name as its input. 

```{r}
# Sort titanic in descending order 

titanic_sorted <- arrange(.data = titanic_data,
                          desc(fare)) 

glimpse(titanic_sorted)
```


### Example - Multi-Column Sort

We can also sort by multiple columns, but this creates a chain of **dependence**, in that the first column sort is maintained before the second sort is performed. 

This means that the second sort won't change the order from the first sort.

When this becomes useful is if there are numerous examples of the same value, such as the 0.0000 values in the fare column.

Whilst the position of 0.0000 will not change, the values in the adjacent column will be sorted. Some of those paying Â£0 fare may have been younger than others, after all. 

```{r}
# Multi-column sort

titanic_double_sort <- arrange(.data = titanic_data,
                               fare,
                               age_of_passenger)

glimpse(titanic_double_sort)
```

Notice that we were able to just continually reference columns from the data as if they were arguments to the function, and the verb understands that immediately.

This is part of the power of these functions, and why **.data** is recommended for use.


### Exercise
::: {.panel-tabset}

### **Exercise**{-}

Sort the titanic data set by age in **descending** order, then fare in **ascending** order.

### **Show Answer**{-}


```{r}
# Sort by age desc, fare asc

titanic_double_sort <- arrange(.data = titanic_data,
                               desc(age_of_passenger),
                                  fare)

glimpse(titanic_double_sort)
```
:::


## Select

Sometimes we will want to work with smaller tibbles that contain just a subset of available columns.

The **select()** verb is perfect for this, and is dplyr's more feature rich answer to the **$** operator from base R. We will endeavour to continue our use of the tidyverse from here, for consistent and readable code. 

It takes the arguments:

* The first is our dataset, made even simpler with ".data"
* From here we list as many columns by name as we would like to be retained in our selecting process.
    * The order in which we specify them is the order they will be in the smaller tibble.

### Example - Single Column

```{r}
# Selecting data

titanic_one_col <- select(.data = titanic_data, 
                           name_of_passenger)
# To display the data

glimpse(titanic_one_col)
```

Notice that this has returned a tibble with one column, not the vector itself that formed that column as **$** would have.

### Example - Multiple Columns

If we want to select multiple columns, we can continue to list them, separating with commas, as we did with **arrange()**.

```{r}
# Selecting data

titanic_three_cols <- select(.data = titanic_data,
                             name_of_passenger, 
                             age_of_passenger,
                             pclass)

# To display the data

glimpse(titanic_three_cols)
```


### Example - Columns in a range

We can select consecutive columns (next to one another) with the syntax we used for indexing in chapter 2, the colon **:** operator.

```{r}
# Selecting from passenger class to the age of passenger in order

titanic_consecutive_cols <- select(.data = titanic,
                                   pclass:age_of_passenger)

glimpse(titanic_consecutive_cols)

```

Notice that we return the columns from pclass to age_of_passenger in the order of the source data. 

### Selecting with **exclusion**

Up to this point we have selected with **inclusion**, a.k.a we specify the columns we want to include in our smaller tibble.

The real power of select comes from it's flexibility, in that we can reduce the amount of code to write by using **exclusion** instead, a.k.a specify the columns we don't want to include.

Say we want to keep 7 of 9 columns, instead of typing out the names of all 7, we can just exclude the 2 we exclude, as they are equivalent processes. 

### **Example**{-}

To exclude, we use the minus sign operator **-** which signifies "do not select this" or "select, not this".

```{r}
# Selecting by excluding columns we don't want

titanic_exclude_cols <- select(.data = titanic_data, 
                               -name_of_passenger, 
                               -age_of_passenger, 
                               -pclass)



glimpse(titanic_exclude_cols)

```

There is an even more streamlined way to do this, by wrapping the columns in a vector with the **c()** function and using one minus sign outside it.

You can think of this as expanding brackets in mathematics:

> "-c(col1, col2) = c(-col1, -col2) = -col1, -col2

```{r}
# Using a vector for exclusion

titanic_exclude_cols <- select(.data = titanic_data, 
                               -c(name_of_passenger, 
                                  age_of_passenger, 
                                  pclass))

glimpse(titanic_exclude_cols)
```

### Selecting with Index Position

We can also select the columns using their index position, starting from 1, just like we did with data structures in chapter 2.

Let's select columns 1 to 4, and also column 7.

```{r}
# Selecting sequential and out of sequence columns with index position

titanic_index_select <- select(.data = titanic_data, 
                               1:4, 
                               7)

# To display the data

glimpse(titanic_index_select)
```

Exclusion works similarly here with the minus sign. 



### Select Helper Functions

As if **select()** wasn't already helpful enough, it even has helper functions that allow us to select on specific patterns, such as a prefix or suffix in a column name. 

They are as follows:

* starts_with(match): Starts with a prefix.
* ends_with(match): Ends with a suffix.
* contains(match): Contains a literal string.
* matches(match): Matches a [regular expression](https://www.regular-expressions.info/quickstart.html).
* everything(): Selects all variables.
* last_col(): Selects the last column.


These can be passed instead of column names **and** alongside selected column names as well. 

### **Examples**{-}

Let's return columns that start with an "s".

```{r}
# Selecting columns

titanic_prefix_select <- select(.data = titanic_data, 
                                starts_with("s"))

# To Display the data

glimpse(titanic_prefix_select)
```

As a second example, let's return everything.

```{r}
# Selecting everything

titanic_every_select <- select(.data = titanic_data,
                               everything())

glimpse(titanic_every_select)

```


### Exercise
::: {.panel-tabset}

### **Exercise**{-}


1. Select the second, third and fourth columns from titanic_data, without typing all three.


2. Select all columns except "fare", "cabin" and "embarked" from titanic_data. Note that these are consecutive.


3. Select just the last column from titanic_data using a helper function.


4. Select the columns that end in the suffix "passenger" using a helper function.


### **Show Answer**{-}

1. 
```{r}
# Select second, third and fourth column

titanic_select_exercise <-  select(.data = titanic_data, 
                                   survived:sex_of_passenger)

glimpse(titanic_select_exercise)

```

2. 
```{r}
# Using exclusion on a range

titanic_select_exercise <- select(.data = titanic_data, 
                                  -fare:-embarked)

# OR

# titanic_select_exercise <- select(.data = titanic_data,
#                                 -c(fare, cabin, embarked))

# OR

# titanic_select_exercise <- select(.data = titanic_data,
#                                   -fare,
#                                   -cabin,
#                                   -embarked)

glimpse(titanic_select_exercise)

```

3. 
```{r}
# Selecting last column only

titanic_select_exercise <- select(.data = titanic_data,
                                  last_col()) 

glimpse(titanic_select_exercise)

```


4.
```{r}
# Selecting on a suffix

titanic_select_exercise <- select(.data = titanic_data,
                                  ends_with("passenger")) 


glimpse(titanic_select_exercise)

```
:::

For more advanced ways to select columns:

* Refer to this excellent blog [Data Wrangling Part 1: Basic to Advanced Ways to Select Columns](https://suzan.rbind.io/2018/01/dplyr-tutorial-1/)


## The Pipe Operator 

Up until now, we have run verbs one at a time on our dataset, be it to sort or select columns. 

The real power of the tidyverse comes from the ability to chain these functions together in a sequence for more complex data manipulation tasks. 

However, out of the box, this becomes laborious quickly due to the nature of functions. They are an application of the mathematical function, so they will take an input, do something to it and then return an output.

However, to apply many functions to a single output, known as **composition** of functions, we must nest them, with the function applied last being written first, and evaluated from the inside to the outside.

### **Example**{-}

For example, let's take the **sqrt()** and **round()** mathematical functions and apply them together, so that we obtain the square root of a number and hence round it to a specified number of decimal places.

```{r}
# Compose mathematical functions

round(sqrt(2))

```
Notice that the functions are evaluated in reverse order (from the inside, out) just as you would with writing this mathematically. You would perform the square root operation first, **then** round the result. 

Should we wish to use an extra parameter, we need to start thinking about the brackets, as they are the opening and closing doors of a function itself.

Thus, the **digits** parameter needs to go inside the round() brackets, not the sqrt() ones.

```{r}
# Compose mathematical functions with extra parameters

round(sqrt(2), digits = 2)

```

Notice that the input to the function was the output from another function. 


### Composition of functions in the tidyverse

This becomes tricky to comprehend when working with the tidyverse functions, as we are effectively performing actions in sequence, such as selecting the columns the want and **then** sorting the result. 

As such, the output of one function is the input to another, as in the following example:

```{r}
# Select passenger columns and fare then arrange by fare

titanic_demographics_sort <- arrange(.data = select(.data = titanic_data,
                                                    ends_with("passenger"),
                                                    fare),
                                     desc(fare))

glimpse(titanic_demographics_sort)
```

Notice that the input to arrange was the result of a select operation, so we had to keep track of our brackets and ensure that the desc(fare) was placed within arrange(), **not** select().

Going forward, we will see more verbs that can be applied on top of this, and it becomes laborious and difficult to manage quickly. This is where the pipe operator comes in.


### Introducing the Pipe

The Pipe Operator makes it possible to chain a sequence of functions starting with the base data as an input to the sequence.

This removes the need to type the **.data** argument each time. 

* Prior to R 4.1, the pipe operator came from a package known s **magrittr**, which would be loaded alongside the tidyverse, and it took the form **%>>%**.

* From R 4.1 onwards, the operator comes as standard with base R, as it has become the universal standard for data analysis. It now takes the form **|>**, and this option must be turned on in the Tools --> Global Options tab. 

![](../Images/native_pipe.png){fig-alt="The Code, Editing pane with the native pipe operator tick box."}

The shortcut for this operator is **CTRL + SHIFT + M** and is one you will use alot from here on. 

### **Example - Single Value**{-}

Let's return to our square root and rounding functions. Let's apply one, and then both in sequence using our new operator.

First, we write the value or dataset we wish to apply functions to, it is at the entrance to the "pipe" if you will. 

Then, we write the function we wish to use and any additional parameters we may need. The pipe operator **passes** the input to the function, so we do not need to write it within the brackets.

```{r}
# Our first pipe - with a single value

2 |> sqrt()

```

You can read the pipe operator as "AND THEN". 

It takes the output of one action "AND THEN" uses that as the input of the next action, and so on. 

This means that to chain a sequence of functions, we will use the pipe operator again to pipe our output to our next function as its input. 

```{r}
# Our first sequence of pipes

2 |> sqrt() |> 
     round(digits = 2)
```
This is quite the shift from what we have done so far, but is more readable in plain english: 

>**To the value 2, apply the square root function, and then, round that output to 2 decimal places.**

Notice, that following the second pipe, we insert a new line, which automatically moves us in a few spaces. This is optional but allows code to read as if it were a list. 


### Using the Pipe Operator with the tidyverse

Let's return to our previous example, where we selected passenger demographics and sorted by fare. 

Let's try to do this with pipes instead.

First, we write the dataset we want to apply this sequence of steps to, then pipe that into the **select()** function, specifying what columns we wish to retain.

Then, we pipe that output to the **arrange()** function, so that it can sort by the column we wish, in the order we wish.

```{r}
# Repeat earlier operation with pipes

titanic_data |> 
    select(ends_with("passenger"),
           fare) |> 
    arrange(desc(fare))
```

This way, the code is written in the order that it executes, as opposed to the reverse order without the pipe operator. 

>**To the titanic dataset, we select columns that end with "passenger" as well as the fare column, and then, sort that in descending order of fare paid.**

We could also pipe the output to a glimpse() if we do not wish to save the output.

```{r}
# Pipeline with glimpse

titanic_data |> 
    select(ends_with("passenger"),
           fare) |> 
    arrange(desc(fare)) |> 
    glimpse()

```


### **Variables with tidyverse chains**{-}

Notice that I didn't create a variable in the prior conversion, since it can cause confusion between the assignment operator **<-** and the pipe operator **|>** if not covered separately. 

In front of the previous sequence, known as a **pipeline**, we would write our variable name and assignment operator, which saves the final output in memory.

```{r}
# Save pipeline as a variable 

titanic_demographics_sort <- titanic_data |> 
    select(ends_with("passenger"),
           fare) |> 
    arrange(desc(fare))


titanic_demographics_sort |> 
    glimpse()
```

Be careful not to confuse the meaning of the two operators, this will take some practice to get used to. 


### Exercise
::: {.panel-tabset}

### **Exercise**{-}

Speaking of pratice, let's try out our new tool!

Create a pipeline that:
* Selects the first five columns of the data
* Arranges them in ascending order of age.

Assign this output to a new variable and glimpse it.


### **Show Answer**{-}

```{r}
# Selecting and ordering by age with pipes

titanic_age_sort <- titanic_data |> 
    select(pclass:age_of_passenger) |> 
    arrange(age_of_passenger)

titanic_age_sort |> 
    glimpse()

```

:::


## Filter

Often we are only interesting in groups of rows that adhere to a specific condition, such as:

* Passengers that paid over or under a certain fare.
* Passengers who are in a particular age threshold.
* Passengers who embarked from Southampton.

and of course, combinations of the above.

The next verb, **filter()** allows us to subset our rows in this way. To understand this section, we first need to consider conditions.


### Conditional Statements

A conditional statement is one that returns **TRUE** or **FALSE** dependent on the outcome. We saw examples of these back in Chapter 1, when we were producing logicals with comparisons. For example:

* 4 < 5 is a condition statement that evaluates to TRUE
* 4 != 4 is a conditional statement that evaluates to FALSE

A reminder of the operators we use are


Logical Operator| Description
:--------------:|:------------:
        <       | Less Than
       <=       | Less Than or Equal To
       \>       | Greater Than
       >=       | Greater Than or Equal To
       ==       | Equal To
       !=       | Not Equal To
       \|       | Or
        &       | And
        !       | Not
      any()     | Checks if any value in a logical vector are TRUE
      all()     | Checks if all values in a logical vector are TRUE
      is.na()   | Is the value missing (NA)?
      between() | Is between 2 numbers


>**Note: The `!` allows us to flip or invert an expression. Basically, if an expression returns `[TRUE, TRUE, FALSE]`, the same expression with a `!` in front of it will return `[FALSE, FALSE, TRUE]`.**

These statements can of course be much more complex than comparing two single numbers, we can apply them across columns with dplyr's **filter()** verb! 


### Single Conditional Filtering

We will first look at filtering by a single condition, which are constructed as follows:

* We begin with the column to compare with.
* Next is the logical operator of choice, such as **<**, **>**.
* Last up is the value to compare each entry in the column to, which generates the set of TRUEs and FALSEs.

This is generated in the background, and **filter()** will keep only the rows that return **TRUE** from this comparison. 

> For example, to subset down to only second class passengers, we would write: "pclass == 2". 

### **Example**{-}

```{r}
# Filter to retain only second class passengers

titanic_second_class <- titanic_data |> 
    filter(pclass == 2)


titanic_second_class |> 
    glimpse()
```

A quick way to check that the filter has worked is to use base R's **unique()** function on the column. 

We need to use our **$** here to reference columns by name as this is not a tidyverse method.

Let's check the original data first.

```{r}
# Return unique values from the original data

unique(titanic_data$pclass)

```

We see that the pclass column has 3 unique levels of 1, 2 and 3.

```{r}
# Return unique values from the filtered data

unique(titanic_second_class$pclass)

```

We see that in the filtered data, we have just second class passengers, as expected. 

### **Example - Numeric**{-}

Let's filter to passengers who paid above £200.

```{r}
# Select passengers who paid more than 200

titanic_data |> 
    filter(fare > 200) |> 
    glimpse()

```

Notice that we didn't save this as a variable, as this was just an example, one we will not carry forward in our analysis. 

However, if you are being asked to answer specific questions, then saving the outputs of your manipulation pipelines as variables is very useful, to prevent overwriting the base data.


### Exercise
::: {.panel-tabset}

### **Exercise**{-}

1. Use filter to return the row for the passenger named: 'Birkeland, Mr. Hans Martin Monsen'

2. Filter for passengers that are male and save it as a variable. Can you count how many there were?

3. Filter for passengers that are under 18 years of age and save it as a variable. Can you count how many there were?

4. **Extension - Harder**: What percentage of passengers in the dataset survived? Remember, to compute a percentage, we must multiply the proportion by 100. 


### **Show Answer**{-}

1. 
```{r}
# Filtering to a specific passenger

titanic_data |> 
    filter(name_of_passenger == 
               'Birkeland, Mr. Hans Martin Monsen') |> 
    glimpse()

```

2. 

To display the number of males in the filtered data, we simply need the number of rows, the length of the data!

```{r}
# Males only

titanic_male <- titanic_data |> 
    filter(sex_of_passenger == 'male')

# Counting the number of males

nrow(titanic_male)
```

3. 

To display the number of passengers below 18 years of age in the filtered data, we simply need the number of rows, the length of the data!

```{r}
# Underage passengers 

titanic_underage <- titanic_data |> 
    filter(age_of_passenger < 18)


# Counting the number of underage passengers

nrow(titanic_filter_exercise)
```


### **Extension Exercise**{-}

4. 

Firstly, we need to filter the dataset to those who survived, and the percentage would be calculated as:

> Number of those who survived/Number of passengers as a whole * 100

Remember, we can compute the number who survived and the number of passengers, we just need the number of rows in each subsequent tibble.

```{r}
# Passengers who survived

titanic_survived <- titanic_data |> 
    filter(survived == 1)

# Survival percentage

(nrow(titanic_survived) / nrow(titanic_data)) * 100

```
So we see that, tragically, only 38% of the passengers aboard the titanic survived.

:::


### Multiple Conditional Filtering 

We have thus far filtered on conditions for a single column, but there is no reason we can't use multiple conditions to filter by several conditions and/or columns at once. 

For example:

* We want **male** passengers who **survived**.
* We want **over 18** passengers who embarked from **Southampton**.

However, we do need to think about how the conditions relate to each other, as we have two options to establish these relationships.


### Relationships between conditions

There are two ways we can combine conditions together:

* **AND** relationships are given by the **&** symbol. 
    * This implies both/all conditions must be met for a row to evaluate to TRUE before the filter is applied.
    * The **between()** function is a special case of this, ensuring our numerical values are above a lower bound, and below an upper bound. So between(3.5, 7.5) retains rows with values above 3.5 and below 7.5.

* **OR** relationships are given by the **|** symbol. 
    * This implies that if any of the conditions can be met (one or more) a given row evaluates to TRUE before the filter is applied.
    * The **%in%** operator is a special case of this, where rows have to contain a value occurring in a given vector (collection) of possible values. This prevents us from writing out an **or** condition for each of the possibilities if just one is enough. 
    * For example, %in% c=(1, 2, 3) retains rows that have the value of 1, 2 of 3 in the compared column.

> This does mean that combinations of TRUE and FALSE conditions can lead to different outputs with AND/OR relationships. This is summarised in the table below.

| Condition 1 | Condition 2  | AND Equates to     | OR Equates to      |
|:-----------:|:------------:|:------------------:|:------------------:|
|     True    |     True     |        True        |        True        |
|     True    |     False    |        False       |        True        |
|    False    |     True     |        False       |        True        |
|    False    |     False    |        False       |        False       | 


### **Examples**{-}

For our first example, let's filter to first class, female passengers. 

This is an AND relationship, as they must be first class **AND** female. Thus, any row that does not satisfy both of these conditions will be filtered out in the process. 

Remember that AND combinations are very strict, so ensure that the loss of a possibly large amount of data is appropriate for your analysis.

```{r}
# Filter to first class female passengers

titanic_data |> 
    filter(pclass == 1 & 
               sex_of_passenger == "female") |> 
    glimpse()

```

For our next example, let's select passengers who were male **OR** adults (over 18). 

This means that males under 18 will be kept and adult females will also be kept, as only one of the conditions needs to be true to be retained by filter!

```{r}
# Filter to males or adults

titanic_data |> 
    filter(sex_of_passenger == "male" |
               age_of_passenger > 18) |> 
    glimpse()

```


### **Special Cases of AND/OR**{-}

Whilst the above conventions are easy to read from left to right, when constructing much more complex chains of conditions, it becomes laborious quickly. 

This is particularly the case if we wish to apply multiple conditions on the **same** column, and hence combine that with other conditions across other columns.

To alleviate this, dplyr has some special functions that allow us to streamline cases of numerous conditions.


#### **The between() function**{-}

First is the **between()** function, which is similarly to an and relationship for conditions on numeric columns.

It allows us to specify:

* An upper bound for the value in the column, a.k.a the value for which it should not be larger.
* A lower bound for the value in the column, a.k.a the value for which it should not be smaller.

> For example, between(3.5 and 7.5) will retain values for which the column is greater than or equal to 3.5 and less than or equal to 7.5.

Let's filter to those that paid between £250 and £500 for their tickets.

```{r}
# Filter to those that paid between two values

titanic_data |> 
    filter(between(fare,
                   left = 250,
                   right = 500)) |> 
    glimpse()

```

Which is equivalent to saying:

> filter(fare >= 250 & fare <= 500)

Of course, we can combine this with other conditions across columns as well. 

```{r}
# Filter tho those that paid between £250 and £500 who were female

titanic_data |> 
    filter(between(fare,
                   left = 250,
                   right = 500) &
               sex_of_passenger == "female") |> 
    glimpse()

```

#### **The %in% function**{-}

Secondly is the **%in%** function, which checks for membership of the column value in a vector of options we provide. 

This is similar to an OR relationship, as it allows us to bring through multiple values from a column.

Let's filter to capture those that embarked from Southampton or Cherbourg.

```{r}
# Filter to those that embarked from S or C

titanic_data |> 
    filter(embarked %in% c("S", "C")) |> 
    glimpse()

```
We can check with **distinct()** whether this accomplished what we were looking for.

```{r}
# Filter to those that embarked from S or C and check

titanic_data |> 
    filter(embarked %in% c("S", "C")) |> 
    distinct(embarked)

```

This process would be equivalent to performing:

> filter(embarked == "S" | embarked == "C") 

and is incredibly useful for larger categorical variables we wish to trim. 

As with between(), we can also combine this with other conditions.

```{r}
# Select passengers embarking from S or C that were male

titanic_data |> 
    filter(embarked  %in% c("S", "C"),
           sex_of_passenger == "male") |> 
    glimpse()

```

### Negating Conditions

We can also use the negation operator **!** to reverse the outcome of the condition. 

This means that if a condition evaluates to TRUE, if we preceed it with an exclamation mark, it will instead return FALSE.

This is useful for cases where exlcusion is quicker than typing out conditions for inclusion. Say for example you have a categorical variable with 9 unique categories.

You are interested in filtering so that only 8 of the categories remain. Instead of chaining **OR** conditions together, you can instead write the condition that would return **just** that one, and negate it to remove it instead.

### **Example**{-}

Let's see this in action by negating the statement to select passengers who embarked from Southampton, as opposed to writing out filters to retain Cherbourg and Queenstown. 

This isn't laborious for this small dataset, but this is a great time saving measure for more complex datasets.

```{r}
# Use negation to filter down to passengers embarking from Cherbourg or Queenstown

titanic_data |> 
    filter(!embarked == "S") |> 
    distinct(embarked)

```

This is, of course equivalent to using the not equal **!=** comparison in such a simple case:

```{r}
# Filter for passengers who did not embark from Southampton

titanic_data |> 
    filter(embarked != "S") |> 
    distinct(embarked)

```

Of course, if you negate and **and**/**or** relationships:

* Not only is each individual condition negated
* The and/or is reversed to or/and respectively

This comes from logical statements in mathematics, specifically, [De Morgan's laws.

![](../Images/demorgans_law.png){fig-alt="De morgans law of negation, where negating a combined statement negates each individual statement, as well as the logical operator combining them."}

#### **More Difficult Example**{-}

Let's take an example we we required those that embarked from Southhampon or paid above £100 in fare.

Mathematically speaking, if we negate this condition, we get:

> !(southampton OR > 100 fare) = (!southampton AND !>100 fare), so we get those that did not embark from Southampton and did not pay more than £100. 

```{r}
# Harder example

titanic_data |> 
    filter(!(embarked == "S" | fare > 100)) |> 
    distinct(embarked)

```
We see that we have indeed negated the selection of Southampton, as expected.


### Exercise
::: {.panel-tabset}

### **Exercise**{-}

1. Filter so that only second or third class passengers are included in the data. 

2. Filter so that only passengers who travelled alone are included in the data. How many were there?


3. **Extension Exercise** - What percentage of passengers who embarked from Cherbourg or Queenstown and paid a fare more than £100, but less than £350, survived? 


### **Show Answer**{-}

1. 
```{r}
# Filter for second or third class passengers

titanic_data |> 
    filter(pclass  %in% c(2, 3)) |> 
    glimpse()

```


2. 

For those travelling alone, their sibsp and parch values should be 0.

```{r}
# Filtering for passengers travelling alone

lone_passengers <- titanic_data |> 
    filter(sibsp == 0 &
               parch == 0) |> 
    glimpse()

# How many there are

nrow(lone_passengers)
```


3. 

This is a multi stage problem, we can filter for all conditions, then compute the final percentage. Since we need all of these conditions to be TRUE, we must use **&** to combine them.

```{r}
# Filter for those that embarked from C or Q, paid certain fare and survived

cherb_queens_survivors <- titanic_data |> 
    filter(embarked %in% c("C", "Q") &
               between(fare, left = 100, right = 350) &
               survived == 1)

# Compute final percentage

round((nrow(cherb_queens_survivors) / nrow(titanic_data)) * 100, digits = 2)

```
:::


## Mutate

To create a new variable, or a new column, in our `tibble` we will use the **`dplyr::mutate()`** function.

Code to create a new column follows the pattern below, where we state the name of the new column and the use the = to state what we want to be in that column.

```{r, eval = F}
# generating new variables

data_frame_name <- mutate(data_frame,
                                 name_of_new_column = contents of new column)
```


### Constant Value Variables

The simplest example would be adding a constant value to each row in our `tibble`. 

```{r}
# generating new variables

mutate_example <- mutate(.data = titanic, 
                                string_twos = "two")

```


### Creating Variables Based on Existing Columns

We can perform mathematical operations and even functions on existing columns in the **`mutate()`** function using column names.

Let's look at some examples:

We can capitalise the sex column.

```{r}
# Capitalise sex column
# Here we are over writing the  sex_of_passenger column
# If you give your new column the same name  as  an existing column 
# It will overwrite it.

mutate_example <- mutate(.data = titanic,
                                sex_of_passenger = str_to_upper(sex_of_passenger))

```


Another example is creating the family size of the person.

This is the number of siblings or spouses on board (sibsp), the number of parents or children on board (parch), plus 1 (the person themselves).

```{r}
# Determining family size on board

mutate_example <- mutate(.data = titanic,
                                family_size = parch + sibsp + 1)
```


As well as mathematical operators and functions we can use conditionals. 

```{r}
# Creating a logical column which is TRUE 
# if passenger is female, false otherwise.

mutate_example <- mutate(.data = titanic,
                                is_female = (sex_of_passenger == "female"))


```


The key note from this being, with **`mutate()`**, we can do almost anything to every row in our data frame. 

There are some functions which work with **`mutate()`** and enable us to perform more complicated tasks. **`recode()`** lets us map one value to another throughout the entire column e.g.:

```{r, eval = FALSE}
# Recoding variables
# Note Recode does not have a .data argument

recode(titanic$survived, 
              '1' = TRUE, 
              '0' = FALSE)
```

To do this in the context of an entire `tibble` we have to use **`recode()`** with **`mutate()`**.

```{r}
# Generating new variables

recode_example <- mutate(.data = titanic,
                                survived_logical = recode(survived, '1' = TRUE, '0' = FALSE))
```


### Exercise
::: {.panel-tabset}

### **Exercise**{-}

1. Create a new column, called `fare_dollars`, in this column we should have the fare converted to dollars, the exchange rate is Â£1 to $1.39.

2. Round the `fare-dollars` column


### **Show Answer**{-}

1. 
```{r}
# Generating new variables

fare_dollars <- mutate(.data = titanic,
                              fare_dollars = fare * 1.39)
```


2. Round the `fare_dollars` column.

```{r}
# Generating new variables

round_fare <- mutate(.data = titanic,
                            fare_dollars = round( fare * 1.39))
```
:::


For more advanced ways to mutate columns refer to this excellent blog [Data Wrangling Part 1: Basic to Advanced Ways to Mutate Columns](https://suzan.rbind.io/2018/02/dplyr-tutorial-2/), which showcases dplyr functions from the mutate() family.



### Deleting data

We can also use the **`mutate()`** function to delete data.

We can add a new column called `child` to our titanic data set.

We can quickly see the names of our data set using the the **`names()`** function.

```{r}
# Generating new variables

titanic_mutate_exercise <- mutate(.data = titanic,
                                         child = age_of_passenger < 18)
# To display the column names

names(titanic_mutate_exercise)
```

We can delete the column using the code below.

We take the titanic data set and set equal the column we want to delete to `NULL`.

```{r}
# Setting a column = NULL effectively removes it.

titanic_mutate_exercise <- mutate(.data = titanic_mutate_exercise, 
                                         child = NULL)

# finding out the names of the columns in the dataset

names(titanic_mutate_exercise)
```

Alternatively, this will work too.

```{r, eval = F}

# selecting a column using the $ sign and assigning 
# it to NULL

titanic_mutate_exercise$child <- NULL
```


## Joining Data

Often all the data you need to answer a question are not contained within a single dataset, but across several. Datasets can be joined, or 'merged', to allow data to be analysed together, but only **if the two datasets share a common reference or identifier.**

Linking data come in a number of forms and are commonly referred to as 'indexical' data. Some examples include:

* Your NHS number, allowing data linkage across the NHS for primary, secondary, tertiary care episodes and prescribing.
* Any account number (e.g. banking, utilities, travel card, council tax etc.) can acts as a point of linkage between different sets of data.
* Your email, phone number, social media handles etc.
* Your address can also act as a spatial reference, linking you to your neighbourhood, local services etc


### Types of Joins

In R we use a group of `join` functions. These are prefixed by the type of join we choose to implement.

Once you have established that two `tibbles` share a reference that will permit a join to be conducted, you will see:

* `inner_join` - Only rows with reference values that appear in both `tibbles` are merged.
* `full_join` - All data from the left and right `tibble` is retained, matched up where possible.
* `left_join` - All data from the 'left' `tibble` is retained, and any matching rows are merged from the 'right' `tibble`.
* `right_join` - All data from the right and anything that matches from the left. Effectively, the reverse of 'Left'.


This can be easier to understand graphically:


![](../Images/sql-joins.png){fig-alt = "Venn diagrams for each of the prior examples, with the included data shaded."}


Lets look at some examples.

We will use the 2 datasets, which come as part of the `dplyr` package. These data sets describe band members of the Beatles and Rolling Stones. 

```{r}
# To display the data

band_members
```

You will need to assign it to see it within the environment window as shown below,

```{r}
# Assigning the data

band_members <- band_members
```


```{r}
# To display the data

band_instruments
```

Lets assign this one as well. Note that `dplyr` has two slightly different datasets (band_instruments and band_instruments2 - which we will use later on). 

```{r}
# Assigning the data

band_instruments <- band_instruments
```


These data sets have one common column which is `name`, so we can join them based on this.

We can see that **John** and **Paul** appear in both datasets while **Mick** is appears only in the band member and **Keith** in band instruments.

Iâ€™m going to say that `band_members` is my â€œleftâ€ data frame, and that `band_instruments` is my â€œrightâ€ data frame.

Letâ€™s look at how we do an **inner join** in code. Weâ€™re going to use the function **`inner_join()`**.


```{r}
# The left data frame is given by the argument x
# The right frame is given by the argument y
# We need to include the column we would like to join with

band_membership_inner_join <- inner_join(x = band_members, 
                                                y = band_instruments,
                                                by = "name")
# To display the data

band_membership_inner_join
```

We can see that the inner join returned only the 2 members that appear in both datasets.


If you don't specify which column to join by, `R` will look to match by any column that matches.


We can also use the `pipe operator`, as shown below.

```{r}
# Joining data using the pipe operator
# Here the left dataset is specified first
# Then we can chain our join to it

band_membership_inner_join <- band_members %>% 
                              inner_join(band_instruments, 
                                                by = "name")

# To display the data

band_membership_inner_join
```
 
 
Now let try a **full join**.

```{r}
# full join

band_membership_full_join <- band_members %>% 
                             full_join(band_instruments,
                                              by = "name")

# To display the data

band_membership_full_join
```

We can see that the full join returned all 4 members with some missing values for those that don't appear in both datasets.


Now lets try a **left join**.

```{r}
# left join

band_membership_left_join <- band_members %>% 
                             left_join(band_instruments, 
                                              by = "name")

# To display the data

band_membership_left_join
```

We can see that the left join returns only those that appear or both datasets or left dataset, which is why Keith is missing here.


A **right join** is just the opposite of the left join.

```{r}
# Right join

band_membership_right_join <- band_members %>% 
                              right_join(band_instruments, 
                                                by = "name")

# To display the data

band_membership_right_join
```

We can see that the right join returns only those that appear or both datasets or right dataset, which is why Mick is missing here.


If we're joining on more than one column we can specify using the parameter `by = c(vector))`.

This is shown below.

```{r, eval = F}

# Joining using multiple columns

join_left <- left_join(x = first_dataset, 
                              y = second_dataset,
                              by = c("column name", "column name"))


```


### Exercise
::: {.panel-tabset}

### **Exercise**{-}

1. Run the code below, can you figure out why its not working. **HINT:Remember what we need to be able to join 2 datasets.**

We will be using band_instruments2 from the `dplyr` package for this exercise, rather than band_instruments as previously. 

```{r, eval=F}
# Load the data

band_members <- band_members

band_instruments2 <- band_instruments2

# Full join

band_membership_full_join <- band_members %>% 
                              full_join(band_instruments2)

# To display the data

band_membership_full_join
```


2. Using `band_members` and `band_instruments2` perform a "full" join.

   * Join on *BOTH* the column containing `name` and the column containing `artist`.
  
   * Note that the names of these columns may not be the same in both data frames.

   * Use the help function to explore solutions to this. **HINT: look at the example section**


### **Show Answer**{-}

1. 
```{r, eval=F}
# Load the data
  # Note later code will run without this step, it has been included to demonstrate that datasets from packages are available 

band_members <- band_members

band_instruments2 <- band_instruments2

# Full join

band_membership_exercise <- band_members %>% 
                             full_join(band_instruments2)

# To display the data

band_membership_exercise
```

The code above will give the error message below;

**Error: `by` must be supplied when `x` and `y` have no common variables. i use by = character()` to perform a cross-join. Run `rlang::last_error()` to see where the error occurred.**

This is because the datasets do not have any columns with the same name, band_members has name while band_instruments2 has artist.

```{r}
# To display the data

band_members
```

```{r}
# To display the data

band_instruments2
```


2. 
```{r}
# Full join
# We can join with different column names

band_membership_exercise <- band_members %>% 
                              full_join(band_instruments2,
                                               by = c("name" = "artist"))

# To display the data

band_membership_exercise
```


### bind_rows()

Another common way in which datasets are combined is by binding them. Unlike the join function, the binding functions do not try to match by a variable, but instead simply combine datasets by appending one set of data to the bottom of the other. If the datasets donâ€™t match by the appropriate dimensions, one obtains an error.

This is where we can add data on to the bottom of an existing data frame, If you have two (or more) data frames that share the same data structure.

You can use the **bind_rows()** function.

```{r}
# Creating vectors

name <- c("Paul","Katherine", "Almas", "Pragya")

band <- c("Beatles","Beatles", "Stones", "Stones")

# Creating a tibble

new_band_members <- tibble(name, band)
```

For example, looking at the 2 data frames below;

```{r}
# To display the data

band_members

# To display the data

new_band_members

```


We can use the **`bind_rows()`** function, which will simply merge two data frames into one by appending the rows.

```{r}
# Combining 2 dataframes using bind_rows() function

binded_rows_data <- band_members %>% 
                    bind_rows(new_band_members)

# To display the data

binded_rows_data

```

We can see our new additions - Paul, Katherine, Almas and Pragya here now.

Here weâ€™re assigning the output to a new Data Frame â€“ `binded_rows_data.


### Union()

Another useful function is  **`union()`** which will merge the data from both data frames but keep only the distinct (unique) rows. That means, any duplicated rows that exist in the target data frames are not going to be brought over to the current data frame

```{r}
# Combining 2 dataframes using union() function

union_data <- band_members %>% 
              union(new_band_members)

# To display the data

union_data

```

We can see that Paul was duplicated and is not included when we use union instead.


# Summary 

We covered alot of content around dplyr here, but the possibilities are nearly endless with this incredible data manipulation library.

By no means are you meant to remember and immediately recall the above. Instead, what is most important is that you understand the problem you are trying to solve, and can use the resources here (or your own) to then solve it.


Next up, we will look at aggregating data and obtaining summary statistics.

